{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed8888-48b1-4090-b085-a9d2d7fbd16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Starting experiment 10/13\n",
      "Configuration: {'num_epochs': 50, 'batch_size': 64, 'learning_rate': 0.001, 'optimizer': 'adam', 'dropout': 0.25, 'crop_scale': (0.8, 1.0), 'flip_prob': 0.3, 'rotate_limit': 15, 'brightness_limit': 0.2, 'contrast_limit': 0.2}\n",
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1145/1145 [00:54<00:00, 21.04it/s, Loss=0.0152, Acc=67.6]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 53.82it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:54<00:00, 21.13it/s, Loss=0.00859, Acc=82.7]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 52.41it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:56<00:00, 20.15it/s, Loss=0.00744, Acc=85.2]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 54.49it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:56<00:00, 20.09it/s, Loss=0.0068, Acc=86.6] \n",
      "Evaluating: 100%|██████████| 407/407 [00:09<00:00, 43.54it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:52<00:00, 21.81it/s, Loss=0.00631, Acc=87.7]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 56.29it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:55<00:00, 20.79it/s, Loss=0.00605, Acc=88.3]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 52.77it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:53<00:00, 21.41it/s, Loss=0.00581, Acc=88.8]\n",
      "Evaluating: 100%|██████████| 407/407 [00:08<00:00, 48.29it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:52<00:00, 21.67it/s, Loss=0.0056, Acc=89.2] \n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 57.69it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:53<00:00, 21.53it/s, Loss=0.00538, Acc=89.8]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 58.06it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:56<00:00, 20.34it/s, Loss=0.00522, Acc=89.9]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 52.44it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:54<00:00, 20.82it/s, Loss=0.00507, Acc=90.4]\n",
      "Evaluating: 100%|██████████| 407/407 [00:08<00:00, 48.84it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:51<00:00, 22.22it/s, Loss=0.00493, Acc=90.7]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 55.84it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:52<00:00, 21.65it/s, Loss=0.00485, Acc=90.7]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 55.01it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:56<00:00, 20.14it/s, Loss=0.0048, Acc=90.8] \n",
      "Evaluating: 100%|██████████| 407/407 [00:08<00:00, 49.81it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:55<00:00, 20.56it/s, Loss=0.00466, Acc=91.2]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 57.10it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:51<00:00, 22.43it/s, Loss=0.00463, Acc=91.3]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 60.06it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:55<00:00, 20.75it/s, Loss=0.00459, Acc=91.2]\n",
      "Evaluating: 100%|██████████| 407/407 [00:08<00:00, 48.59it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:51<00:00, 22.15it/s, Loss=0.00441, Acc=91.6]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 61.54it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:50<00:00, 22.52it/s, Loss=0.00443, Acc=91.7]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 62.86it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:48<00:00, 23.62it/s, Loss=0.00436, Acc=91.7]\n",
      "Evaluating: 100%|██████████| 407/407 [00:08<00:00, 50.38it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:53<00:00, 21.55it/s, Loss=0.00433, Acc=91.8]\n",
      "Evaluating: 100%|██████████| 407/407 [00:09<00:00, 43.96it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:49<00:00, 23.04it/s, Loss=0.00426, Acc=92]  \n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 63.07it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:50<00:00, 22.84it/s, Loss=0.00414, Acc=92.1]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 60.30it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:49<00:00, 23.33it/s, Loss=0.00418, Acc=92.2]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 54.88it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:50<00:00, 22.51it/s, Loss=0.00415, Acc=92.2]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 56.57it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:53<00:00, 21.41it/s, Loss=0.0041, Acc=92.3] \n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 56.54it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:49<00:00, 23.31it/s, Loss=0.00407, Acc=92.3]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 58.21it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:47<00:00, 24.04it/s, Loss=0.00399, Acc=92.4]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 57.60it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:49<00:00, 23.23it/s, Loss=0.00398, Acc=92.5]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 60.64it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:50<00:00, 22.79it/s, Loss=0.00397, Acc=92.5]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 58.51it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:51<00:00, 22.24it/s, Loss=0.00387, Acc=92.7]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 61.53it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:50<00:00, 22.54it/s, Loss=0.0039, Acc=92.6] \n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 58.71it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:50<00:00, 22.74it/s, Loss=0.00385, Acc=92.7]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 58.75it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:47<00:00, 24.27it/s, Loss=0.00383, Acc=92.8]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 59.01it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:50<00:00, 22.75it/s, Loss=0.00382, Acc=92.8]\n",
      "Evaluating: 100%|██████████| 407/407 [00:08<00:00, 50.84it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:49<00:00, 23.02it/s, Loss=0.00376, Acc=92.8]\n",
      "Evaluating: 100%|██████████| 407/407 [00:08<00:00, 46.79it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:48<00:00, 23.50it/s, Loss=0.00376, Acc=92.9]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 52.78it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:53<00:00, 21.38it/s, Loss=0.00377, Acc=92.9]\n",
      "Evaluating: 100%|██████████| 407/407 [00:09<00:00, 43.12it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:51<00:00, 22.07it/s, Loss=0.00372, Acc=93]  \n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 54.77it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:50<00:00, 22.71it/s, Loss=0.00364, Acc=93.2]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 61.48it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:49<00:00, 23.14it/s, Loss=0.00368, Acc=93] \n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 58.12it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:48<00:00, 23.38it/s, Loss=0.00365, Acc=93.1]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 59.87it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:50<00:00, 22.82it/s, Loss=0.00362, Acc=93.1]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 58.30it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:51<00:00, 22.34it/s, Loss=0.0036, Acc=93.2] \n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 60.61it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:49<00:00, 23.08it/s, Loss=0.00359, Acc=93.2]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 60.44it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:47<00:00, 23.92it/s, Loss=0.00358, Acc=93.3]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 60.95it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:49<00:00, 22.96it/s, Loss=0.00351, Acc=93.3]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 61.87it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:51<00:00, 22.38it/s, Loss=0.00355, Acc=93.3]\n",
      "Evaluating: 100%|██████████| 407/407 [00:07<00:00, 52.32it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:53<00:00, 21.36it/s, Loss=0.00353, Acc=93.3]\n",
      "Evaluating: 100%|██████████| 407/407 [00:06<00:00, 61.84it/s]\n",
      "Training: 100%|██████████| 1145/1145 [00:48<00:00, 23.68it/s, Loss=0.00345, Acc=93.4]\n",
      "Evaluating: 100%|██████████| 407/407 [00:08<00:00, 48.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting experiment 11/13\n",
      "Configuration: {'num_epochs': 55, 'batch_size': 128, 'learning_rate': 0.0005, 'optimizer': 'rmsprop', 'dropout': 0.3, 'crop_scale': (0.85, 1.0), 'flip_prob': 0.25, 'rotate_limit': 12, 'brightness_limit': 0.15, 'contrast_limit': 0.15}\n",
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 573/573 [00:49<00:00, 11.63it/s, Loss=0.00909, Acc=61]  \n",
      "Evaluating: 100%|██████████| 204/204 [00:07<00:00, 28.06it/s]\n",
      "Training: 100%|██████████| 573/573 [00:50<00:00, 11.27it/s, Loss=0.00462, Acc=81.3]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 30.94it/s]\n",
      "Training: 100%|██████████| 573/573 [00:45<00:00, 12.65it/s, Loss=0.00394, Acc=84.2]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 32.49it/s]\n",
      "Training: 100%|██████████| 573/573 [00:46<00:00, 12.43it/s, Loss=0.00353, Acc=86.2]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 31.08it/s]\n",
      "Training: 100%|██████████| 573/573 [00:46<00:00, 12.42it/s, Loss=0.00331, Acc=86.9]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 33.44it/s]\n",
      "Training: 100%|██████████| 573/573 [00:44<00:00, 12.97it/s, Loss=0.00309, Acc=87.9]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 31.40it/s]\n",
      "Training: 100%|██████████| 573/573 [00:50<00:00, 11.34it/s, Loss=0.00298, Acc=88.5]\n",
      "Evaluating: 100%|██████████| 204/204 [00:07<00:00, 25.53it/s]\n",
      "Training: 100%|██████████| 573/573 [00:49<00:00, 11.56it/s, Loss=0.00287, Acc=89]  \n",
      "Evaluating: 100%|██████████| 204/204 [00:07<00:00, 28.93it/s]\n",
      "Training: 100%|██████████| 573/573 [00:48<00:00, 11.79it/s, Loss=0.00277, Acc=89.3]\n",
      "Evaluating: 100%|██████████| 204/204 [00:07<00:00, 28.76it/s]\n",
      "Training: 100%|██████████| 573/573 [00:48<00:00, 11.76it/s, Loss=0.00269, Acc=89.7]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 30.83it/s]\n",
      "Training: 100%|██████████| 573/573 [00:47<00:00, 12.03it/s, Loss=0.00263, Acc=90]  \n",
      "Evaluating: 100%|██████████| 204/204 [00:07<00:00, 28.83it/s]\n",
      "Training: 100%|██████████| 573/573 [00:46<00:00, 12.28it/s, Loss=0.00254, Acc=90.3]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 29.43it/s]\n",
      "Training: 100%|██████████| 573/573 [00:47<00:00, 12.05it/s, Loss=0.0025, Acc=90.5] \n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 30.36it/s]\n",
      "Training: 100%|██████████| 573/573 [00:47<00:00, 12.19it/s, Loss=0.00244, Acc=90.8]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 31.89it/s]\n",
      "Training: 100%|██████████| 573/573 [00:48<00:00, 11.78it/s, Loss=0.00237, Acc=90.9]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 32.33it/s]\n",
      "Training: 100%|██████████| 573/573 [00:49<00:00, 11.69it/s, Loss=0.00234, Acc=91.1]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 30.11it/s]\n",
      "Training: 100%|██████████| 573/573 [00:46<00:00, 12.41it/s, Loss=0.00235, Acc=91.1]\n",
      "Evaluating: 100%|██████████| 204/204 [00:07<00:00, 28.41it/s]\n",
      "Training: 100%|██████████| 573/573 [00:52<00:00, 10.98it/s, Loss=0.00227, Acc=91.3]\n",
      "Evaluating: 100%|██████████| 204/204 [00:07<00:00, 25.75it/s]\n",
      "Training: 100%|██████████| 573/573 [00:52<00:00, 10.84it/s, Loss=0.00223, Acc=91.7]\n",
      "Evaluating: 100%|██████████| 204/204 [00:07<00:00, 26.97it/s]\n",
      "Training: 100%|██████████| 573/573 [00:48<00:00, 11.87it/s, Loss=0.00222, Acc=91.6]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 29.25it/s]\n",
      "Training: 100%|██████████| 573/573 [00:45<00:00, 12.66it/s, Loss=0.00218, Acc=91.7]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 32.60it/s]\n",
      "Training: 100%|██████████| 573/573 [00:43<00:00, 13.16it/s, Loss=0.00215, Acc=92]  \n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 32.44it/s]\n",
      "Training: 100%|██████████| 573/573 [00:48<00:00, 11.90it/s, Loss=0.0021, Acc=92.1] \n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 29.23it/s]\n",
      "Training: 100%|██████████| 573/573 [00:48<00:00, 11.87it/s, Loss=0.00207, Acc=92.2]\n",
      "Evaluating: 100%|██████████| 204/204 [00:05<00:00, 34.47it/s]\n",
      "Training: 100%|██████████| 573/573 [00:52<00:00, 10.97it/s, Loss=0.00208, Acc=92.1]\n",
      "Evaluating: 100%|██████████| 204/204 [00:05<00:00, 34.52it/s]\n",
      "Training: 100%|██████████| 573/573 [00:47<00:00, 12.11it/s, Loss=0.00209, Acc=92.2]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 33.44it/s]\n",
      "Training: 100%|██████████| 573/573 [00:53<00:00, 10.79it/s, Loss=0.00205, Acc=92.3]\n",
      "Evaluating: 100%|██████████| 204/204 [00:07<00:00, 28.68it/s]\n",
      "Training: 100%|██████████| 573/573 [00:45<00:00, 12.52it/s, Loss=0.00201, Acc=92.5]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 32.68it/s]\n",
      "Training: 100%|██████████| 573/573 [00:43<00:00, 13.08it/s, Loss=0.00199, Acc=92.6]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 33.50it/s]\n",
      "Training: 100%|██████████| 573/573 [00:47<00:00, 11.95it/s, Loss=0.00198, Acc=92.6]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 29.31it/s]\n",
      "Training: 100%|██████████| 573/573 [00:42<00:00, 13.48it/s, Loss=0.00199, Acc=92.6]\n",
      "Evaluating: 100%|██████████| 204/204 [00:05<00:00, 34.46it/s]\n",
      "Training: 100%|██████████| 573/573 [00:44<00:00, 12.94it/s, Loss=0.00194, Acc=92.8]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 33.72it/s]\n",
      "Training: 100%|██████████| 573/573 [00:48<00:00, 11.71it/s, Loss=0.00193, Acc=92.8]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 32.79it/s]\n",
      "Training: 100%|██████████| 573/573 [00:43<00:00, 13.10it/s, Loss=0.00193, Acc=92.8]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 32.17it/s]\n",
      "Training: 100%|██████████| 573/573 [00:46<00:00, 12.32it/s, Loss=0.00191, Acc=92.9]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 31.52it/s]\n",
      "Training: 100%|██████████| 573/573 [00:44<00:00, 12.78it/s, Loss=0.00191, Acc=92.9]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 33.84it/s]\n",
      "Training: 100%|██████████| 573/573 [00:45<00:00, 12.58it/s, Loss=0.00189, Acc=93]  \n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 31.34it/s]\n",
      "Training: 100%|██████████| 573/573 [00:45<00:00, 12.71it/s, Loss=0.00186, Acc=93.1]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 32.30it/s]\n",
      "Training: 100%|██████████| 573/573 [00:44<00:00, 12.82it/s, Loss=0.00185, Acc=93.1]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 31.05it/s]\n",
      "Training: 100%|██████████| 573/573 [00:46<00:00, 12.43it/s, Loss=0.00184, Acc=93.2]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 29.50it/s]\n",
      "Training: 100%|██████████| 573/573 [00:45<00:00, 12.59it/s, Loss=0.00183, Acc=93.1]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 33.93it/s]\n",
      "Training: 100%|██████████| 573/573 [00:43<00:00, 13.12it/s, Loss=0.00183, Acc=93.2]\n",
      "Evaluating: 100%|██████████| 204/204 [00:05<00:00, 34.25it/s]\n",
      "Training: 100%|██████████| 573/573 [00:46<00:00, 12.39it/s, Loss=0.00181, Acc=93.3]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 32.32it/s]\n",
      "Training: 100%|██████████| 573/573 [00:48<00:00, 11.77it/s, Loss=0.0018, Acc=93.4] \n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 31.44it/s]\n",
      "Training: 100%|██████████| 573/573 [00:46<00:00, 12.36it/s, Loss=0.00178, Acc=93.3]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 32.49it/s]\n",
      "Training: 100%|██████████| 573/573 [00:43<00:00, 13.23it/s, Loss=0.00176, Acc=93.4]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 33.24it/s]\n",
      "Training: 100%|██████████| 573/573 [00:54<00:00, 10.58it/s, Loss=0.00176, Acc=93.5]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 29.93it/s]\n",
      "Training: 100%|██████████| 573/573 [00:51<00:00, 11.17it/s, Loss=0.00179, Acc=93.3]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 29.21it/s]\n",
      "Training: 100%|██████████| 573/573 [00:51<00:00, 11.11it/s, Loss=0.00177, Acc=93.5]\n",
      "Evaluating: 100%|██████████| 204/204 [00:07<00:00, 28.29it/s]\n",
      "Training: 100%|██████████| 573/573 [00:47<00:00, 11.95it/s, Loss=0.00175, Acc=93.4]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 33.38it/s]\n",
      "Training: 100%|██████████| 573/573 [00:43<00:00, 13.17it/s, Loss=0.00174, Acc=93.6]\n",
      "Evaluating: 100%|██████████| 204/204 [00:05<00:00, 34.28it/s]\n",
      "Training: 100%|██████████| 573/573 [00:44<00:00, 12.98it/s, Loss=0.00173, Acc=93.6]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 31.11it/s]\n",
      "Training: 100%|██████████| 573/573 [00:44<00:00, 12.81it/s, Loss=0.00171, Acc=93.6]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 31.62it/s]\n",
      "Training: 100%|██████████| 573/573 [00:45<00:00, 12.59it/s, Loss=0.00174, Acc=93.5]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 32.62it/s]\n",
      "Training: 100%|██████████| 573/573 [00:46<00:00, 12.38it/s, Loss=0.00168, Acc=93.8]\n",
      "Evaluating: 100%|██████████| 204/204 [00:06<00:00, 29.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting experiment 12/13\n",
      "Configuration: {'num_epochs': 60, 'batch_size': 256, 'learning_rate': 0.01, 'optimizer': 'sgd', 'dropout': 0.35, 'crop_scale': (0.75, 1.0), 'flip_prob': 0.35, 'rotate_limit': 20, 'brightness_limit': 0.25, 'contrast_limit': 0.25}\n",
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 287/287 [00:42<00:00,  6.80it/s, Loss=0.00748, Acc=33]  \n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.59it/s]\n",
      "Training: 100%|██████████| 287/287 [00:43<00:00,  6.56it/s, Loss=0.00513, Acc=55.3]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.43it/s]\n",
      "Training: 100%|██████████| 287/287 [00:39<00:00,  7.18it/s, Loss=0.00377, Acc=68.4]\n",
      "Evaluating: 100%|██████████| 102/102 [00:05<00:00, 17.75it/s]\n",
      "Training: 100%|██████████| 287/287 [00:39<00:00,  7.21it/s, Loss=0.00317, Acc=73.8]\n",
      "Evaluating: 100%|██████████| 102/102 [00:05<00:00, 17.64it/s]\n",
      "Training: 100%|██████████| 287/287 [00:41<00:00,  6.94it/s, Loss=0.00281, Acc=77]  \n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.07it/s]\n",
      "Training: 100%|██████████| 287/287 [00:43<00:00,  6.56it/s, Loss=0.00256, Acc=79.2]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.04it/s]\n",
      "Training: 100%|██████████| 287/287 [00:42<00:00,  6.74it/s, Loss=0.00243, Acc=80.6]\n",
      "Evaluating: 100%|██████████| 102/102 [00:05<00:00, 17.15it/s]\n",
      "Training: 100%|██████████| 287/287 [00:47<00:00,  6.03it/s, Loss=0.00229, Acc=81.5]\n",
      "Evaluating: 100%|██████████| 102/102 [00:05<00:00, 17.27it/s]\n",
      "Training: 100%|██████████| 287/287 [00:48<00:00,  5.91it/s, Loss=0.00217, Acc=82.7]\n",
      "Evaluating: 100%|██████████| 102/102 [00:07<00:00, 13.33it/s]\n",
      "Training: 100%|██████████| 287/287 [00:42<00:00,  6.69it/s, Loss=0.0021, Acc=83.4] \n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.00it/s]\n",
      "Training: 100%|██████████| 287/287 [00:46<00:00,  6.16it/s, Loss=0.00203, Acc=84.1]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.10it/s]\n",
      "Training: 100%|██████████| 287/287 [00:47<00:00,  6.03it/s, Loss=0.00197, Acc=84.6]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.24it/s]\n",
      "Training: 100%|██████████| 287/287 [00:43<00:00,  6.59it/s, Loss=0.00191, Acc=85.1]\n",
      "Evaluating: 100%|██████████| 102/102 [00:07<00:00, 13.54it/s]\n",
      "Training: 100%|██████████| 287/287 [00:47<00:00,  6.00it/s, Loss=0.00187, Acc=85.4]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.27it/s]\n",
      "Training: 100%|██████████| 287/287 [00:48<00:00,  5.93it/s, Loss=0.00183, Acc=85.7]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.08it/s]\n",
      "Training: 100%|██████████| 287/287 [00:43<00:00,  6.52it/s, Loss=0.00179, Acc=86.1]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.13it/s]\n",
      "Training: 100%|██████████| 287/287 [00:43<00:00,  6.61it/s, Loss=0.00173, Acc=86.6]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.27it/s]\n",
      "Training: 100%|██████████| 287/287 [00:43<00:00,  6.58it/s, Loss=0.0017, Acc=86.9] \n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.53it/s]\n",
      "Training: 100%|██████████| 287/287 [00:46<00:00,  6.22it/s, Loss=0.00168, Acc=87]  \n",
      "Evaluating: 100%|██████████| 102/102 [00:07<00:00, 13.62it/s]\n",
      "Training: 100%|██████████| 287/287 [00:57<00:00,  5.01it/s, Loss=0.00166, Acc=87.1]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.53it/s]\n",
      "Training: 100%|██████████| 287/287 [00:42<00:00,  6.69it/s, Loss=0.00164, Acc=87.4]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.93it/s]\n",
      "Training: 100%|██████████| 287/287 [00:42<00:00,  6.69it/s, Loss=0.00162, Acc=87.5]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.71it/s]\n",
      "Training: 100%|██████████| 287/287 [00:43<00:00,  6.59it/s, Loss=0.0016, Acc=87.7] \n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.63it/s]\n",
      "Training: 100%|██████████| 287/287 [00:43<00:00,  6.63it/s, Loss=0.00159, Acc=88]  \n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.22it/s]\n",
      "Training: 100%|██████████| 287/287 [00:43<00:00,  6.53it/s, Loss=0.00157, Acc=88]  \n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.11it/s]\n",
      "Training: 100%|██████████| 287/287 [00:46<00:00,  6.21it/s, Loss=0.00154, Acc=88.3]\n",
      "Evaluating: 100%|██████████| 102/102 [00:07<00:00, 13.99it/s]\n",
      "Training: 100%|██████████| 287/287 [00:50<00:00,  5.69it/s, Loss=0.00154, Acc=88.3]\n",
      "Evaluating: 100%|██████████| 102/102 [00:07<00:00, 13.61it/s]\n",
      "Training: 100%|██████████| 287/287 [00:45<00:00,  6.25it/s, Loss=0.0015, Acc=88.4] \n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.47it/s]\n",
      "Training: 100%|██████████| 287/287 [00:46<00:00,  6.21it/s, Loss=0.00149, Acc=88.6]\n",
      "Evaluating: 100%|██████████| 102/102 [00:07<00:00, 13.39it/s]\n",
      "Training: 100%|██████████| 287/287 [00:47<00:00,  6.10it/s, Loss=0.00149, Acc=88.7]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.38it/s]\n",
      "Training: 100%|██████████| 287/287 [00:45<00:00,  6.35it/s, Loss=0.00148, Acc=88.8]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.75it/s]\n",
      "Training: 100%|██████████| 287/287 [00:43<00:00,  6.63it/s, Loss=0.00147, Acc=88.9]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.61it/s]\n",
      "Training: 100%|██████████| 287/287 [00:43<00:00,  6.55it/s, Loss=0.00144, Acc=89]  \n",
      "Evaluating: 100%|██████████| 102/102 [00:05<00:00, 17.29it/s]\n",
      "Training: 100%|██████████| 287/287 [00:43<00:00,  6.58it/s, Loss=0.00142, Acc=89.1]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.45it/s]\n",
      "Training: 100%|██████████| 287/287 [00:44<00:00,  6.47it/s, Loss=0.00143, Acc=89]  \n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.48it/s]\n",
      "Training: 100%|██████████| 287/287 [00:46<00:00,  6.15it/s, Loss=0.00141, Acc=89.2]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.21it/s]\n",
      "Training: 100%|██████████| 287/287 [00:44<00:00,  6.44it/s, Loss=0.00141, Acc=89.3]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.28it/s]\n",
      "Training: 100%|██████████| 287/287 [00:46<00:00,  6.14it/s, Loss=0.0014, Acc=89.3] \n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.58it/s]\n",
      "Training: 100%|██████████| 287/287 [00:48<00:00,  5.94it/s, Loss=0.0014, Acc=89.6] \n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.76it/s]\n",
      "Training: 100%|██████████| 287/287 [00:47<00:00,  6.08it/s, Loss=0.00138, Acc=89.6]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.52it/s]\n",
      "Training: 100%|██████████| 287/287 [00:46<00:00,  6.20it/s, Loss=0.00137, Acc=89.7]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.86it/s]\n",
      "Training: 100%|██████████| 287/287 [00:47<00:00,  6.04it/s, Loss=0.00137, Acc=89.6]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.44it/s]\n",
      "Training: 100%|██████████| 287/287 [00:49<00:00,  5.76it/s, Loss=0.00136, Acc=89.6]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.37it/s]\n",
      "Training: 100%|██████████| 287/287 [00:46<00:00,  6.18it/s, Loss=0.00134, Acc=89.9]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.77it/s]\n",
      "Training: 100%|██████████| 287/287 [00:45<00:00,  6.29it/s, Loss=0.00135, Acc=89.9]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.17it/s]\n",
      "Training: 100%|██████████| 287/287 [00:51<00:00,  5.63it/s, Loss=0.00133, Acc=90]  \n",
      "Evaluating: 100%|██████████| 102/102 [00:07<00:00, 14.16it/s]\n",
      "Training: 100%|██████████| 287/287 [00:46<00:00,  6.23it/s, Loss=0.00131, Acc=90.1]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.10it/s]\n",
      "Training: 100%|██████████| 287/287 [00:46<00:00,  6.14it/s, Loss=0.00132, Acc=90.1]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.52it/s]\n",
      "Training: 100%|██████████| 287/287 [00:43<00:00,  6.66it/s, Loss=0.00131, Acc=90.2]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.24it/s]\n",
      "Training: 100%|██████████| 287/287 [00:49<00:00,  5.80it/s, Loss=0.0013, Acc=90.2] \n",
      "Evaluating: 100%|██████████| 102/102 [00:07<00:00, 14.32it/s]\n",
      "Training: 100%|██████████| 287/287 [00:44<00:00,  6.47it/s, Loss=0.00128, Acc=90.3]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.70it/s]\n",
      "Training: 100%|██████████| 287/287 [00:50<00:00,  5.68it/s, Loss=0.00129, Acc=90.3]\n",
      "Evaluating: 100%|██████████| 102/102 [00:07<00:00, 14.01it/s]\n",
      "Training: 100%|██████████| 287/287 [00:46<00:00,  6.19it/s, Loss=0.0013, Acc=90.2] \n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 14.72it/s]\n",
      "Training: 100%|██████████| 287/287 [00:44<00:00,  6.49it/s, Loss=0.00129, Acc=90.4]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 14.98it/s]\n",
      "Training: 100%|██████████| 287/287 [00:46<00:00,  6.20it/s, Loss=0.00127, Acc=90.4]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 14.65it/s]\n",
      "Training: 100%|██████████| 287/287 [00:44<00:00,  6.46it/s, Loss=0.00127, Acc=90.5]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 15.56it/s]\n",
      "Training: 100%|██████████| 287/287 [00:45<00:00,  6.26it/s, Loss=0.00127, Acc=90.6]\n",
      "Evaluating: 100%|██████████| 102/102 [00:06<00:00, 16.43it/s]\n",
      "Training: 100%|██████████| 287/287 [00:41<00:00,  6.92it/s, Loss=0.00127, Acc=90.6]\n",
      "Evaluating: 100%|██████████| 102/102 [00:05<00:00, 17.08it/s]\n",
      "Training: 100%|██████████| 287/287 [00:40<00:00,  7.03it/s, Loss=0.00126, Acc=90.7]\n",
      "Evaluating: 100%|██████████| 102/102 [00:05<00:00, 17.67it/s]\n",
      "Training: 100%|██████████| 287/287 [00:41<00:00,  6.95it/s, Loss=0.00126, Acc=90.5]\n",
      "Evaluating: 100%|██████████| 102/102 [00:05<00:00, 17.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment Results:\n",
      "\n",
      "experiment_10:\n",
      "Best accuracy: 96.26%\n",
      "Configuration: {'num_epochs': 50, 'batch_size': 64, 'learning_rate': 0.001, 'optimizer': 'adam', 'dropout': 0.25, 'crop_scale': (0.8, 1.0), 'flip_prob': 0.3, 'rotate_limit': 15, 'brightness_limit': 0.2, 'contrast_limit': 0.2}\n",
      "\n",
      "experiment_11:\n",
      "Best accuracy: 96.10%\n",
      "Configuration: {'num_epochs': 55, 'batch_size': 128, 'learning_rate': 0.0005, 'optimizer': 'rmsprop', 'dropout': 0.3, 'crop_scale': (0.85, 1.0), 'flip_prob': 0.25, 'rotate_limit': 12, 'brightness_limit': 0.15, 'contrast_limit': 0.15}\n",
      "\n",
      "experiment_12:\n",
      "Best accuracy: 95.62%\n",
      "Configuration: {'num_epochs': 60, 'batch_size': 256, 'learning_rate': 0.01, 'optimizer': 'sgd', 'dropout': 0.35, 'crop_scale': (0.75, 1.0), 'flip_prob': 0.35, 'rotate_limit': 20, 'brightness_limit': 0.25, 'contrast_limit': 0.25}\n"
     ]
    }
   ],
   "source": [
    "# The model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Memory management function\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Training history tracking class\n",
    "class TrainingHistory:\n",
    "    def __init__(self):\n",
    "        self.history = {\n",
    "            'epoch': [],\n",
    "            'learning_rate': [],\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'test_loss': [],\n",
    "            'test_acc': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1_score': [],\n",
    "            'timestamp': [],\n",
    "            'test_probs': [],\n",
    "            'test_labels': [],\n",
    "            'confusion_matrix_path': []\n",
    "        }\n",
    "\n",
    "    def update(self, epoch, lr, train_loss, train_acc, test_loss, test_acc, metrics, probs=None, labels=None):\n",
    "        self.history['epoch'].append(epoch)\n",
    "        self.history['learning_rate'].append(lr)\n",
    "        self.history['train_loss'].append(float(train_loss))\n",
    "        self.history['train_acc'].append(float(train_acc))\n",
    "        self.history['test_loss'].append(float(test_loss))\n",
    "        self.history['test_acc'].append(float(test_acc))\n",
    "        self.history['precision'].append(metrics['precision'])\n",
    "        self.history['recall'].append(metrics['recall'])\n",
    "        self.history['f1_score'].append(metrics['f1'])\n",
    "        self.history['timestamp'].append(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        self.history['confusion_matrix_path'].append(f'confusion_matrix_epoch_{epoch}.png')\n",
    "\n",
    "        if probs is not None and labels is not None:\n",
    "            self.history['test_probs'].append([prob.tolist() for prob in probs])\n",
    "            self.history['test_labels'].append([int(label) for label in labels])\n",
    "\n",
    "    def save(self, file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(self.history, f, indent=4)\n",
    "\n",
    "# Data Processing and Augmentation\n",
    "class SVHNDataset(Dataset):\n",
    "    def __init__(self, split='train', transform=None):\n",
    "        self.data = datasets.SVHN(root='./data', split=split, download=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        image = np.array(image)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Data augmentation class\n",
    "class SVHNAugmentation:\n",
    "    def __init__(self):\n",
    "        self.train_transform = A.Compose([\n",
    "            A.RandomResizedCrop(height=32, width=32, scale=(0.8, 1.0)),\n",
    "            A.HorizontalFlip(),\n",
    "            A.Rotate(limit=20),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "            A.Normalize(mean=[0.4377, 0.4438, 0.4728], std=[0.1980, 0.2010, 0.1970]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "        self.test_transform = A.Compose([\n",
    "            A.Normalize(mean=[0.4377, 0.4438, 0.4728], std=[0.1980, 0.2010, 0.1970]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def get_train_transform(self):\n",
    "        return self.train_transform\n",
    "\n",
    "    def get_test_transform(self):\n",
    "        return self.test_transform\n",
    "\n",
    "# Define the VGG model\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.25):\n",
    "        super(VGG, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 2),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, save_path=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    plt.clf()\n",
    "\n",
    "# Training function for one epoch\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc='Training')\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix({'Loss': running_loss / total, 'Acc': 100. * correct / total})\n",
    "        \n",
    "    return running_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1': float(f1)\n",
    "    }\n",
    "    \n",
    "    return avg_loss, accuracy, all_probs, all_labels, metrics\n",
    "\n",
    "def run_experiment(config_id, params):\n",
    "    try:\n",
    "        num_epochs = params['num_epochs']\n",
    "        batch_size = params['batch_size']\n",
    "        learning_rate = params['learning_rate']\n",
    "        optimizer_type = params['optimizer']\n",
    "        dropout_rate = params['dropout']\n",
    "        \n",
    "        augmentation = SVHNAugmentation()\n",
    "        augmentation.train_transform = A.Compose([\n",
    "            A.RandomResizedCrop(height=32, width=32, scale=params['crop_scale']),\n",
    "            A.HorizontalFlip(p=params['flip_prob']),\n",
    "            A.Rotate(limit=params['rotate_limit']),\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=params['brightness_limit'],\n",
    "                contrast_limit=params['contrast_limit'],\n",
    "                p=0.5\n",
    "            ),\n",
    "            A.Normalize(mean=[0.4377, 0.4438, 0.4728], \n",
    "                       std=[0.1980, 0.2010, 0.1970]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        history = TrainingHistory()\n",
    "        \n",
    "        train_dataset = SVHNDataset(split='train', \n",
    "                                  transform=augmentation.get_train_transform())\n",
    "        test_dataset = SVHNDataset(split='test', \n",
    "                                 transform=augmentation.get_test_transform())\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                shuffle=True, num_workers=0)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                               shuffle=False, num_workers=0)\n",
    "        \n",
    "        model = VGG(dropout_rate=dropout_rate).to(device)\n",
    "        \n",
    "        if optimizer_type == 'adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        elif optimizer_type == 'sgd':\n",
    "            optimizer = optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                                momentum=0.9)\n",
    "        elif optimizer_type == 'rmsprop':\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "            \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        os.makedirs(f'./models/experiment_{config_id}', exist_ok=True)\n",
    "        \n",
    "        best_acc = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, \n",
    "                                              criterion, optimizer)\n",
    "            test_loss, test_acc, probs, labels, metrics = evaluate(\n",
    "                model, test_loader, criterion\n",
    "            )\n",
    "            \n",
    "            history.update(epoch + 1, learning_rate, train_loss, train_acc, \n",
    "                         test_loss, test_acc, metrics, probs=probs, \n",
    "                         labels=labels)\n",
    "            \n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_acc': best_acc,\n",
    "                    'config': params\n",
    "                }, f'./models/experiment_{config_id}/best_model.pth')\n",
    "            \n",
    "            history.save(f'./models/experiment_{config_id}/training_history.json')\n",
    "        \n",
    "        return best_acc\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in experiment {config_id}: {e}\")\n",
    "        raise\n",
    "\n",
    "# 10 different configurations\n",
    "configurations_Low_dropout = [\n",
    "    {\n",
    "    'num_epochs': 50,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.001,\n",
    "    'optimizer': 'adam',  \n",
    "    'dropout': 0.25,\n",
    "    'crop_scale': (0.8, 1.0),\n",
    "    'flip_prob': 0.3,    \n",
    "    'rotate_limit': 15,  # 適度旋轉\n",
    "    'brightness_limit': 0.2,\n",
    "    'contrast_limit': 0.2\n",
    "}\n",
    "]\n",
    "\n",
    "configurations_test = [\n",
    "    {\n",
    "        # Adam配置\n",
    "        'num_epochs': 50,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer': 'adam',\n",
    "        'dropout': 0.25,\n",
    "        'crop_scale': (0.8, 1.0),\n",
    "        'flip_prob': 0.3,\n",
    "        'rotate_limit': 15,\n",
    "        'brightness_limit': 0.2,\n",
    "        'contrast_limit': 0.2\n",
    "    },\n",
    "    {\n",
    "        # RMSprop配置\n",
    "        'num_epochs': 55,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.0005,\n",
    "        'optimizer': 'rmsprop',\n",
    "        'dropout': 0.3,\n",
    "        'crop_scale': (0.85, 1.0),\n",
    "        'flip_prob': 0.25,\n",
    "        'rotate_limit': 12,\n",
    "        'brightness_limit': 0.15,\n",
    "        'contrast_limit': 0.15\n",
    "    },\n",
    "    {\n",
    "        # SGD配置\n",
    "        'num_epochs': 60,\n",
    "        'batch_size': 256,\n",
    "        'learning_rate': 0.01,\n",
    "        'optimizer': 'sgd',\n",
    "        'dropout': 0.35,\n",
    "        'crop_scale': (0.75, 1.0),\n",
    "        'flip_prob': 0.35,\n",
    "        'rotate_limit': 20,\n",
    "        'brightness_limit': 0.25,\n",
    "        'contrast_limit': 0.25\n",
    "    }\n",
    "]\n",
    "\n",
    "configurations = [\n",
    "    {\n",
    "        # Base configuration\n",
    "        'num_epochs': 50,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer': 'adam',\n",
    "        'dropout': 0.25,\n",
    "        'crop_scale': (0.8, 1.0),\n",
    "        'flip_prob': 0.5,\n",
    "        'rotate_limit': 20,\n",
    "        'brightness_limit': 0.2,\n",
    "        'contrast_limit': 0.2\n",
    "    },\n",
    "    {\n",
    "        # High learning rate with SGD\n",
    "        'num_epochs': 60,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.01,\n",
    "        'optimizer': 'sgd',\n",
    "        'dropout': 0.3,\n",
    "        'crop_scale': (0.7, 1.0),\n",
    "        'flip_prob': 0.7,\n",
    "        'rotate_limit': 30,\n",
    "        'brightness_limit': 0.3,\n",
    "        'contrast_limit': 0.3\n",
    "    },\n",
    "    {\n",
    "        # Small batch size with RMSprop\n",
    "        'num_epochs': 40,\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 0.0005,\n",
    "        'optimizer': 'rmsprop',\n",
    "        'dropout': 0.2,\n",
    "        'crop_scale': (0.9, 1.0),\n",
    "        'flip_prob': 0.3,\n",
    "        'rotate_limit': 15,\n",
    "        'brightness_limit': 0.1,\n",
    "        'contrast_limit': 0.1\n",
    "    },\n",
    "    {\n",
    "        # Strong augmentation\n",
    "        'num_epochs': 70,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer': 'adam',\n",
    "        'dropout': 0.35,\n",
    "        'crop_scale': (0.6, 1.0),\n",
    "        'flip_prob': 0.8,\n",
    "        'rotate_limit': 40,\n",
    "        'brightness_limit': 0.4,\n",
    "        'contrast_limit': 0.4\n",
    "    },\n",
    "    {\n",
    "        # Long training\n",
    "        'num_epochs': 100,\n",
    "        'batch_size': 96,\n",
    "        'learning_rate': 0.0008,\n",
    "        'optimizer': 'adam',\n",
    "        'dropout': 0.28,\n",
    "        'crop_scale': (0.75, 1.0),\n",
    "        'flip_prob': 0.6,\n",
    "        'rotate_limit': 25,\n",
    "        'brightness_limit': 0.25,\n",
    "        'contrast_limit': 0.25\n",
    "    },\n",
    "    {\n",
    "        # Minimal batch size\n",
    "        'num_epochs': 45,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 0.0003,\n",
    "        'optimizer': 'adam',\n",
    "        'dropout': 0.15,\n",
    "        'crop_scale': (0.85, 1.0),\n",
    "        'flip_prob': 0.4,\n",
    "        'rotate_limit': 10,\n",
    "        'brightness_limit': 0.15,\n",
    "        'contrast_limit': 0.15\n",
    "    },\n",
    "    {\n",
    "        # Maximum batch size\n",
    "        'num_epochs': 30,\n",
    "        'batch_size': 256,\n",
    "        'learning_rate': 0.002,\n",
    "        'optimizer': 'sgd',\n",
    "        'dropout': 0.4,\n",
    "        'crop_scale': (0.7, 0.9),\n",
    "        'flip_prob': 0.7,\n",
    "        'rotate_limit': 35,\n",
    "        'brightness_limit': 0.35,\n",
    "        'contrast_limit': 0.35\n",
    "    },\n",
    "    {\n",
    "        # Weak augmentation\n",
    "        'num_epochs': 55,\n",
    "        'batch_size': 80,\n",
    "        'learning_rate': 0.0015,\n",
    "        'optimizer': 'rmsprop',\n",
    "        'dropout': 0.22,\n",
    "        'crop_scale': (0.9, 1.0),\n",
    "        'flip_prob': 0.2,\n",
    "        'rotate_limit': 5,\n",
    "        'brightness_limit': 0.1,\n",
    "        'contrast_limit': 0.1\n",
    "    },\n",
    "    {\n",
    "        # High dropout\n",
    "        'num_epochs': 65,\n",
    "        'batch_size': 48,\n",
    "        'learning_rate': 0.0012,\n",
    "        'optimizer': 'adam',\n",
    "        'dropout': 0.5,\n",
    "        'crop_scale': (0.75, 0.95),\n",
    "        'flip_prob': 0.6,\n",
    "        'rotate_limit': 28,\n",
    "        'brightness_limit': 0.28,\n",
    "        'contrast_limit': 0.28\n",
    "    },\n",
    "    {\n",
    "        # Balanced configuration\n",
    "        'num_epochs': 50,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer': 'adam',\n",
    "        'dropout': 0.3,\n",
    "        'crop_scale': (0.8, 1.0),\n",
    "        'flip_prob': 0.5,\n",
    "        'rotate_limit': 25,\n",
    "        'brightness_limit': 0.2,\n",
    "        'contrast_limit': 0.2\n",
    "    }\n",
    "]\n",
    "\n",
    "def main():\n",
    "    results = {}\n",
    "    \n",
    "    for i, config in enumerate(configurations_test):\n",
    "        print(f\"\\nStarting experiment {i+1}/10\")\n",
    "        print(\"Configuration:\", config)\n",
    "        \n",
    "        best_acc = run_experiment(i+1, config)\n",
    "        results[f\"experiment_{i+1}\"] = {\n",
    "            \"config\": config,\n",
    "            \"best_accuracy\": best_acc\n",
    "        }\n",
    "        \n",
    "        with open('./models/experiment_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        clear_memory()\n",
    "        \n",
    "    print(\"\\nExperiment Results:\")\n",
    "    for exp_id, result in results.items():\n",
    "        print(f\"\\n{exp_id}:\")\n",
    "        print(f\"Best accuracy: {result['best_accuracy']:.2f}%\")\n",
    "        print(\"Configuration:\", result['config'])\n",
    "\n",
    "def main_compare_test():\n",
    "    results = {}\n",
    "    \n",
    "    # only run configurations_Low_dropout\n",
    "    for i, config in enumerate(configurations_test):\n",
    "        print(f\"\\nStarting Low Dropout experiment\")\n",
    "        print(\"Configuration:\", config)\n",
    "        \n",
    "        best_acc = run_experiment('Test', config)\n",
    "        results[\"experiment_low_dropout\"] = {\n",
    "            \"config\": config,\n",
    "            \"best_accuracy\": best_acc\n",
    "        }\n",
    "        \n",
    "        with open('./models/Test_experiment_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        clear_memory()\n",
    "        \n",
    "    print(\"\\nExperiment Results:\")\n",
    "    for exp_id, result in results.items():\n",
    "        print(f\"\\n{exp_id}:\")\n",
    "        print(f\"Best accuracy: {result['best_accuracy']:.2f}%\")\n",
    "        print(\"Configuration:\", result['config'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ef88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee85a59-72f5-4fab-b0bf-a9a912d64c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the curves\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, save_path=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    plt.clf()\n",
    "\n",
    "def create_training_plots(history, config_id):\n",
    "    # Create a figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Training and Validation Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history['epoch'], history['train_loss'], 'b-', label='Training Loss')\n",
    "    plt.plot(history['epoch'], history['test_loss'], 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss', fontsize=12)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 2. Training and Validation Accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history['epoch'], history['train_acc'], 'b-', label='Training Accuracy')\n",
    "    plt.plot(history['epoch'], history['test_acc'], 'r-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy', fontsize=12)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 3. Precision, Recall, and F1 Score\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history['epoch'], history['precision'], 'b-', label='Precision')\n",
    "    plt.plot(history['epoch'], history['recall'], 'r-', label='Recall')\n",
    "    plt.plot(history['epoch'], history['f1_score'], 'g-', label='F1 Score')\n",
    "    plt.title('Model Metrics Over Time', fontsize=12)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 4. ROC Curves for last epoch\n",
    "    plt.subplot(2, 2, 4)\n",
    "    \n",
    "    # Get the last epoch's probabilities and labels\n",
    "    last_probs = np.array(history['test_probs'][-1])\n",
    "    last_labels = np.array(history['test_labels'][-1])\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    # Calculate ROC curve for each class\n",
    "    for i in range(10):  # 10 classes for SVHN\n",
    "        fpr, tpr, _ = roc_curve((last_labels == i).astype(int), last_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color=colors[i], label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves (Last Epoch)', fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./models/experiment_{config_id}/training_plots_{config_id}.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def create_class_performance_plot(history, config_id):\n",
    "    # Get the last epoch's probabilities and labels\n",
    "    last_probs = np.array(history['test_probs'][-1])\n",
    "    last_labels = np.array(history['test_labels'][-1])\n",
    "    predictions = np.argmax(last_probs, axis=1)\n",
    "    \n",
    "    # Calculate class-wise metrics\n",
    "    class_accuracy = []\n",
    "    class_precision = []\n",
    "    class_recall = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        class_mask = last_labels == i\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_accuracy.append(np.mean(predictions[class_mask] == i))\n",
    "            class_precision.append(np.sum((predictions == i) & (last_labels == i)) / np.sum(predictions == i))\n",
    "            class_recall.append(np.sum((predictions == i) & (last_labels == i)) / np.sum(last_labels == i))\n",
    "    \n",
    "    # Create bar plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = np.arange(10)\n",
    "    width = 0.25\n",
    "    \n",
    "    ax.bar(x - width, class_accuracy, width, label='Accuracy', color='skyblue')\n",
    "    ax.bar(x, class_precision, width, label='Precision', color='lightgreen')\n",
    "    ax.bar(x + width, class_recall, width, label='Recall', color='salmon')\n",
    "    \n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Class-wise Performance Metrics', fontsize=12)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'Class {i}' for i in range(10)])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./models/experiment_{config_id}/class_performance_{config_id}.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "for config_id in range(10, 13):\n",
    "    # Load the training history\n",
    "    with open(f'./models/experiment_{config_id}/training_history.json', 'r') as f:\n",
    "        history = json.load(f)\n",
    "\n",
    "    # Create the plots\n",
    "    create_training_plots(history, config_id)\n",
    "    create_class_performance_plot(history, config_id)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    last_labels = np.array(history['test_labels'][-1])\n",
    "    last_probs = np.array(history['test_probs'][-1])\n",
    "    predictions = np.argmax(last_probs, axis=1)\n",
    "    \n",
    "    # Save confusion matrix\n",
    "    confusion_matrix_path = f'./models/experiment_{config_id}/confusion_matrix_{config_id}.png'\n",
    "    plot_confusion_matrix(last_labels, predictions, confusion_matrix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10d6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment Results Summary:\n",
      "Experiment ID Accuracy Precision Recall F1 Score Final Train Loss Final Val Loss\n",
      "     Config 0   0.9491    0.9460 0.9461   0.9459           0.2078         0.1880\n"
     ]
    }
   ],
   "source": [
    "# Build the table\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def create_summary_table(all_experiments):\n",
    "    # Initialize lists to store results\n",
    "    results = []\n",
    "    \n",
    "    for config_id, history in all_experiments.items():\n",
    "        # Get the last epoch's results\n",
    "        last_probs = np.array(history['test_probs'][-1])\n",
    "        last_labels = np.array(history['test_labels'][-1])\n",
    "        predictions = np.argmax(last_probs, axis=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(last_labels, predictions)\n",
    "        precision = precision_score(last_labels, predictions, average='macro')\n",
    "        recall = recall_score(last_labels, predictions, average='macro')\n",
    "        f1 = f1_score(last_labels, predictions, average='macro')\n",
    "        \n",
    "        # Get final loss values\n",
    "        final_train_loss = history['train_loss'][-1]\n",
    "        final_val_loss = history['test_loss'][-1]\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Experiment ID': f'Config {config_id}',\n",
    "            'Accuracy': f'{accuracy:.4f}',\n",
    "            'Precision': f'{precision:.4f}',\n",
    "            'Recall': f'{recall:.4f}',\n",
    "            'F1 Score': f'{f1:.4f}',\n",
    "            'Final Train Loss': f'{final_train_loss:.4f}',\n",
    "            'Final Val Loss': f'{final_val_loss:.4f}'\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# Load all experiment results\n",
    "all_experiments = {}\n",
    "for config_id in range(1, 11):\n",
    "    try:\n",
    "        with open(f'./models/experiment_{config_id}/training_history.json', 'r') as f:\n",
    "            all_experiments[config_id] = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No data found for experiment {config_id}\")\n",
    "        continue\n",
    "\n",
    "# Create and display summary table\n",
    "summary_table = create_summary_table(all_experiments)\n",
    "print(\"\\nExperiment Results Summary:\")\n",
    "print(summary_table.to_string(index=False))\n",
    "\n",
    "# Save to CSV (optional)\n",
    "summary_table.to_csv('./experiment_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b6b71b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
